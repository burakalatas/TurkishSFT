from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers datasets trl accelerate

from datasets import load_dataset

ds = load_dataset("TFLai/Turkish-Alpaca")

print(ds)

def format_alpaca(example):
    if example["input"].strip() != "":
        prompt = (
            "### Instruction:\n"
            f"{example['instruction']}\n\n"
            "### Input:\n"
            f"{example['input']}\n\n"
            "### Response:\n"
            f"{example['output']}"
        )
    else:
        prompt = (
            "### Instruction:\n"
            f"{example['instruction']}\n\n"
            "### Response:\n"
            f"{example['output']}"
        )
    return {"text": prompt}

train_dataset_50k = ds["train"]

if len(train_dataset_50k) > 50000:
    train_dataset_50k = train_dataset_50k.shuffle(seed=42).select(range(50000))

train_dataset_50k = train_dataset_50k.map(format_alpaca)

from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))
model.train()

from transformers import TrainingArguments

output_dir_50k = "/content/drive/MyDrive/KolektifProje/Models/Alpaca_50k"

training_args = TrainingArguments(
    output_dir=output_dir_50k,
    per_device_train_batch_size=2,   # ðŸ”´ 50k iÃ§in dÃ¼ÅŸÃ¼rdÃ¼k
    gradient_accumulation_steps=8,   # ðŸ”´ denge
    learning_rate=5e-5,
    num_train_epochs=2,              # ðŸ”´ 3 yerine 2 (yeterli)
    fp16=True,
    logging_steps=100,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none"
)

from trl import SFTTrainer

trainer_50k = SFTTrainer(
    model=model,
    train_dataset=train_dataset_50k,
    args=training_args
)

trainer_50k.train()

trainer_50k.model.save_pretrained(output_dir_50k)
tokenizer.save_pretrained(output_dir_50k)

from transformers import GPT2LMHeadModel, GPT2Tokenizer

output_dir_50k = "/content/drive/MyDrive/KolektifProje/Models/Alpaca_50k"

model_50k = GPT2LMHeadModel.from_pretrained(output_dir_50k).to("cuda")
tokenizer_50k = GPT2Tokenizer.from_pretrained(output_dir_50k)

tokenizer_50k.pad_token = tokenizer_50k.eos_token
model_50k.eval()

import torch

def ask_model(model, tokenizer, question, max_new_tokens=100):
    prompt = f"### Instruction:\n{question}\n\n### Response:\n"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

while True:
    q = input("TÃ¼rkÃ§e soru gir (q Ã§Ä±kÄ±ÅŸ): ")
    if q.lower() == "q":
        break
    print(ask_model(model_50k, tokenizer_50k, q))

