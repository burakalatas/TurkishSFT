from google.colab import drive
drive.mount('/content/drive')

!pip install -q transformers datasets accelerate peft trl

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

MODEL_NAME = "gpt2"

tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to("cuda")
model.eval()

def ask_model(model, tokenizer, question, max_new_tokens=100):
    prompt = f"### Soru:\n{question}\n\n### Cevap:\n"

    inputs = tokenizer(question, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.8,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

  while True:
    q = input("Türkçe soru gir (çıkmak için q): ")
    if q.lower() == "q":
        break
    print(ask_model(model, tokenizer, q))

from datasets import load_dataset

ds = load_dataset("TFLai/Turkish-Alpaca")
ds

def format_alpaca(example):
    if example["input"].strip() == "":
        text = f"### Soru:\n{example['instruction']}\n\n### Cevap:\n{example['output']}"
    else:
        text = f"""### Soru:
{example['instruction']}
Girdi: {example['input']}

### Cevap:
{example['output']}"""
    return {"text": text}

ds = ds.map(format_alpaca)

  import random

def get_random_subset(dataset, size, seed=42):
    random.seed(seed)
    indices = random.sample(range(len(dataset)), size)
    return dataset.select(indices)

random_2k = get_random_subset(ds["train"], 2000)
random_5k = get_random_subset(ds["train"], 5000)

selected = ds["train"].filter(
    lambda x: len(x["instruction"]) < 200 and "kod" not in x["instruction"].lower()
)

selected_2k = selected.select(range(2000))
selected_5k = selected.select(range(5000))

def tokenize(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

train_dataset = random_2k.map(
    tokenize,
    remove_columns=random_2k.column_names
)

from transformers import TrainingArguments
from trl import SFTTrainer

train_dataset_random_5k = random_5k.map(tokenize, remove_columns=random_5k.column_names)
output_dir = "/content/drive/MyDrive/KolektifProje/Models/Random_5k"

model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

output_dir_random_5k = "/content/drive/MyDrive/KolektifProje/Models/Random_5k"

training_args = TrainingArguments(
    output_dir=output_dir_random_5k,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    num_train_epochs=3,
    fp16=True,
    logging_steps=50,
    save_strategy="epoch",
    report_to="none"
)

trainer_random_5k = SFTTrainer(
    model=model,
    train_dataset=train_dataset_random_5k,
    args=training_args
)

trainer_random_5k.train()

trainer_random_5k.model.save_pretrained(output_dir_random_5k)
tokenizer.save_pretrained(output_dir_random_5k)

output_dir_random_5k = "/content/drive/MyDrive/KolektifProje/Models/Random_5k"

model_5k = GPT2LMHeadModel.from_pretrained(output_dir_random_5k).to("cuda")
tokenizer_5k = GPT2Tokenizer.from_pretrained(output_dir_random_5k)
tokenizer_5k.pad_token = tokenizer_5k.eos_token
model_5k.eval()

while True:
    q = input("Türkçe soru gir (q çıkış): ")
    if q.lower() == "q":
        break
    print(ask_model(model_5k, tokenizer_5k, q))

train_dataset_selected_2k = selected_2k.map(tokenize, remove_columns=selected_2k.column_names)
output_dir = "/content/drive/MyDrive/KolektifProje/Models/Selected_2k"

model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))
model.train()

output_dir_selected_2k = "/content/drive/MyDrive/KolektifProje/Models/Selected_2k"

training_args = TrainingArguments(
    output_dir=output_dir_selected_2k,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    num_train_epochs=3,
    fp16=True,
    logging_steps=50,
    save_strategy="epoch",
    report_to="none"
)

trainer_selected_2k = SFTTrainer(
    model=model,
    train_dataset=train_dataset_selected_2k,
    args=training_args
)

trainer_selected_2k.train()

trainer_selected_2k.model.save_pretrained(output_dir_selected_2k)
tokenizer.save_pretrained(output_dir_selected_2k)

output_dir_selected_2k = "/content/drive/MyDrive/KolektifProje/Models/Selected_2k"

model_selected_2k = GPT2LMHeadModel.from_pretrained(output_dir_selected_2k).to("cuda")
tokenizer_selected_2k = GPT2Tokenizer.from_pretrained(output_dir_selected_2k)
tokenizer_selected_2k.pad_token = tokenizer_selected_2k.eos_token
model_selected_2k.eval()

while True:
    q = input("Türkçe soru gir (q çıkış): ")
    if q.lower() == "q":
        break
    print(ask_model(model_selected_2k, tokenizer_selected_2k, q))

train_dataset_selected_5k = selected_5k.map(tokenize, remove_columns=selected_5k.column_names)
output_dir = "/content/drive/MyDrive/KolektifProje/Models/Selected_5k"

from transformers import TrainingArguments
from trl import SFTTrainer

model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))
model.train()

output_dir_selected_5k = "/content/drive/MyDrive/KolektifProje/Models/Selected_5k"

training_args = TrainingArguments(
    output_dir=output_dir_selected_5k,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    num_train_epochs=3,
    fp16=True,
    logging_steps=50,
    save_strategy="epoch",
    report_to="none"
)

trainer_selected_5k = SFTTrainer(
    model=model,
    train_dataset=train_dataset_selected_5k,
    args=training_args
)

trainer_selected_5k.train()

trainer_selected_5k.model.save_pretrained(output_dir_selected_5k)
tokenizer.save_pretrained(output_dir_selected_5k)

output_dir_selected_5k = "/content/drive/MyDrive/KolektifProje/Models/Selected_5k"

model_selected_5k = GPT2LMHeadModel.from_pretrained(output_dir_selected_5k).to("cuda")
tokenizer_selected_5k = GPT2Tokenizer.from_pretrained(output_dir_selected_5k)
tokenizer_selected_5k.pad_token = tokenizer_selected_5k.eos_token
model_selected_5k.eval()

while True:
    q = input("Türkçe soru gir (q çıkış): ")
    if q.lower() == "q":
        break
    print(ask_model(model_selected_5k, tokenizer_selected_5k, q))
  
